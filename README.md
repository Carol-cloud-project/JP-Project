# Financial Data Prediction using Transformer and Llama3 Embeddings

This repository contains code for predicting financial data using Transformer models and Llama3 embeddings. It is split into two parts: the first part involves using raw financial data, and the second part focuses on converting balance sheet data into text and applying Llama3 embeddings.

## Prerequisites

Before starting, make sure to install all necessary dependencies.

```bash
pip install -r requirements.txt
```

You will also need to apply for a private token on HuggingFace to access the Llama3 model.

---

## Part 1: Transformer-based Prediction

### Step 1: Get the Data

Run the `get_data.py` script to collect the financial data (`X` and `y`) for all companies. This step will prepare the data and store it as `df_X` and `df_y`. These datasets will be saved to GitHub.

```bash
python get_data.py
```

### Step 2: Transformer Model Prediction

Run the `transformer_main.py` script to train and predict the financial data using the Transformer model.

```bash
python transformer_main.py
```

---

## Part 2: Embedding-based Prediction with Llama3

### Step 3: Convert Balance Sheet to Text

Run the `get_text.py` script to convert the balance sheet data into text format, which is required for embedding generation.

```bash
python get_text.py
```

### Step 4: Generate Llama3 Embeddings

Run the `get_embedding.py` script to obtain Llama3 embeddings for the balance sheet text. You will need to provide a private HuggingFace token to access the model.

```bash
python get_embedding.py
```

### Step 5: Transformer Prediction with Embedding Only

Run the `transformer_main_emb_only.py` script to make predictions using the embeddings generated by Llama3.

```bash
python transformer_main_emb_only.py
```

### Step 6: Stacking Predictions

Run the `transformer_main_concat.py` script to combine the Transformer-based predictions and the Llama3 embeddings for stacking. This will predict the next time's balance sheet.

```bash
python transformer_main_concat.py
```

---

## Notes

- The Llama3 model requires authentication via HuggingFaceâ€™s private token. Make sure to request and add your token in the environment variables for the `get_embedding.py` script.
- You can find the generated datasets (`df_X` and `df_y`) and model outputs in the GitHub repository for reference.

